import time
import keras

from keras.models import Sequential
from keras.layers import (
    Activation, Conv2D, Dense,
    Dropout, BatchNormalization, Flatten,
    MaxPooling2D
)
import keras.backend.tensorflow_backend as K

from autoda.data_augmentation import ImageAugmentation

from autoda.networks.utils import (
    _update_history, get_data, get_input_shape,
)
from autoda.networks.architectures import ARCHITECTURES


def train_model(model, train_data, validation_data, data_mean, data_variance,
                batch_size=512, configuration=None,
                time_budget=900, max_epochs=40, ):

    x_train, y_train = train_data
    x_validation, y_validation = validation_data

    train_history, runtime = {}, []

    used_budget, num_epochs, duration_last_epoch = 0., 0, 0.
    num_datapoints, *_ = x_train.shape

    start_time = time.time()

    while(num_epochs < max_epochs) and \
            (used_budget + 1.11 * duration_last_epoch < time_budget):
        if configuration:
            print("Using real-time data augmentation.")

            augmenter = ImageAugmentation(configuration)

            # Fit the model on the batches augmented data generated by apply transform
            history = model.fit_generator(
                augmenter.apply_transform(
                    x_train, y_train,
                    data_mean, data_variance,
                    batch_size=batch_size
                ),
                steps_per_epoch=num_datapoints // batch_size,
                epochs=num_epochs + 1,
                validation_data=(x_validation, y_validation),
                initial_epoch=num_epochs
            )
        else:
            print('Not using data augmentation.')

            history = model.fit(
                x_train, y_train,
                batch_size=batch_size,
                epochs=num_epochs + 1,
                validation_data=(x_validation, y_validation),
                initial_epoch=num_epochs,
                shuffle=True
            )

            train_history = _update_history(train_history, history.history)

        num_epochs += len(history.history.get("loss", []))
        duration_last_epoch = (time.time() - start_time) - used_budget
        used_budget += duration_last_epoch
        print("used_budget", used_budget, "duration_last_epoch", duration_last_epoch, "time_budget", time_budget)
        runtime.append(time.time() - start_time)

    _, validation_accuracy, *_ = model.evaluate(*validation_data, verbose=0)
    print("TRAIN_HIST", train_history)

    result = {
        "train_accuracy": train_history["acc"][-1],
        "validation_loss": train_history["val_loss"][-1],
        "validation_error": 1 - validation_accuracy,
        "used_budget": used_budget,
        "train_history": train_history,
    }

    if configuration:
        result["configs"] = configuration.get_dictionary()
    else:
        result["configs"] = {}

    return result


def train_evaluate(model, train_data, validation_data, data_mean, data_variance,
                   batch_size=512, configuration=None,
                   time_budget=900, max_epochs=40, ):

    runtime, used_budget, train_history = train_model(
        model, train_data, validation_data, data_mean, data_variance,
        batch_size, configuration, time_budget, max_epochs
    )

    # Evaluate model with test data set and share sample prediction results
    # XXX: Figure out where the loss is located in "model.evaluate"
    # return values and also include it in result
    _, validation_accuracy, *_ = model.evaluate(*validation_data, verbose=0)

    result = {
        "train_accuracy": train_history["acc"][-1],
        "validation_loss": train_history["val_loss"][-1],
        "validation_error": 1 - validation_accuracy,
        "used_budget": used_budget,
        "train_history": train_history,
    }

    if configuration:
        result["configs"] = configuration.get_dictionary()
    else:
        result["configs"] = {}

    return result


def objective_function(configuration=None, dataset="cifar10", benchmark="AlexNet", max_epochs=40, batch_size=512):

    augment = configuration is not None

    # preprocess data
    x_train, y_train, x_validation, y_validation, x_test, y_test, data_mean, data_variance = get_data(dataset, augment)

    input_shape = get_input_shape(x_train)  # NWHC

    num_classes = y_train.shape[1]

    train_history, runtime = {}, []

    used_budget, num_epochs, duration_last_epoch = 0., 0, 0.
    num_datapoints, *_ = x_train.shape
    time_budget = 900

    start_time = time.time()

    print("after", num_classes)

    config = K.tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)
    sess = K.tf.Session(config=config)
    K.set_session(sess)

    with K.tf.device("/gpu:1"):
        with sess.graph.as_default():
            # assert benchmark in ARCHITECTURES
            # AlexNet
            # network_function = ARCHITECTURES[benchmark]
            # model = network_function(num_classes=num_classes, input_shape=input_shape)

            model = Sequential()
            model.add(Conv2D(32, (3, 3), padding='same',
                             input_shape=input_shape))

            model.add(BatchNormalization())
            model.add(Activation('relu'))
            model.add(MaxPooling2D(pool_size=(3, 3), strides=2))
            model.add(Conv2D(94, (5, 5)))
            model.add(BatchNormalization())
            model.add(Activation('relu'))
            model.add(MaxPooling2D(pool_size=(3, 3), strides=2))

            model.add(Conv2D(126, (5, 5), padding='same'))
            model.add(BatchNormalization())
            model.add(Activation('relu'))
            model.add(MaxPooling2D(pool_size=(3, 3), strides=2))

            model.add(Flatten())
            model.add(Dense(num_classes))
            model.add(Activation('softmax'))

            # initiate RMSprop optimizer
            # opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)

            opt = keras.optimizers.Adam(lr=0.0016681005372000575)

            # Let's train the model using RMSprop
            model.compile(loss='categorical_crossentropy',
                          optimizer=opt,
                          metrics=['accuracy'])

            while(num_epochs < max_epochs) and \
                    (used_budget + 1.11 * duration_last_epoch < time_budget):
                if configuration:
                    print("Using real-time data augmentation.")

                    augmenter = ImageAugmentation(configuration)

                    # Fit the model on the batches augmented data generated by apply transform
                    history = model.fit_generator(
                        augmenter.apply_transform(
                            x_train, y_train,
                            data_mean, data_variance,
                            batch_size=batch_size
                        ),
                        steps_per_epoch=num_datapoints // batch_size,
                        epochs=num_epochs + 1,
                        validation_data=(x_validation, y_validation),
                        initial_epoch=num_epochs
                    )
                else:
                    print('Not using data augmentation.')

                    history = model.fit(
                        x_train, y_train,
                        batch_size=batch_size,
                        epochs=num_epochs + 1,
                        validation_data=(x_validation, y_validation),
                        initial_epoch=num_epochs,
                        shuffle=True
                    )

                    train_history = _update_history(train_history, history.history)

                num_epochs += len(history.history.get("loss", []))
                duration_last_epoch = (time.time() - start_time) - used_budget
                used_budget += duration_last_epoch
                print("used_budget", used_budget, "duration_last_epoch", duration_last_epoch, "time_budget", time_budget)
                runtime.append(time.time() - start_time)

            validation_loss, validation_accuracy = model.evaluate(x_validation, y_validation, verbose=0)

    print("TRAIN_HIST", train_history)
    result = {
        "validation_loss": validation_loss,
        "validation_error": 1 - validation_accuracy,
        "used_budget": used_budget,
        # "train_history": train_history,
        "configs": configuration
    }

    if configuration:
        result["configs"] = configuration
    else:
        result["configs"] = {}

    return result
