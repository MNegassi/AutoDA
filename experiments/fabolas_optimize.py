#!/usr/bin/python3

from os.path import join as path_join, dirname, realpath
import sys
import time
import numpy as np
import keras

from sklearn import svm

import logging
logging.basicConfig(level=logging.INFO)

from robo.fmin import fabolas

from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D

from sklearn.model_selection import train_test_split

PARENT_DIRECTORY = path_join(dirname(realpath(__file__)), "..")
sys.path.insert(0, PARENT_DIRECTORY)

from autoda.data_augmentation import ImageAugmentation
import ConfigSpace as CS


# Example script to optimize data augmentation parameters for
# LeNet on MNIST with Fabolas.
# See [1] for more details on Fabolas
# [1] " Fast Bayesian Optimization of Machine Learning
# Hyperparameters on Large Datasets" (http://arxiv.org/abs/1605.07079)


def objective_function(x, s, config_space):
    """
    # The optimization function that we want to optimize.
    # It gets a numpy array x with shape (D,) where D are the number of parameters
    # and s which is the ratio of the training data that is used to
    # evaluate this configuration
    """
    start_time = time.time()

    hyperparameters = config_space.get_hyperparameters()

    def hyperparameter_value(x, hyperparameter, hyperparameter_index):
        if isinstance(hyperparameter, CS.UniformFloatHyperparameter):
            value = x[hyperparameter_index]
        else:
            value = int(round(x[hyperparameter_index]))

        return value

    fabolas_mapping = {
        hyperparameter.name: hyperparameter_value(x, hyperparameter, hyperparameter_index)
        for hyperparameter_index, hyperparameter in enumerate(hyperparameters)
    }

    fabolas_config = CS.Configuration(
        configuration_space=config_space, values=fabolas_mapping
    )

    print(fabolas_mapping)
    print(fabolas_config)

    batch_size = 128
    num_classes = 10
    epochs = 12

    # input image dimensions
    img_rows, img_cols = 28, 28

    # The data, shuffled and split between train and test sets:
    (x_train, y_train), (x_test, y_test) = mnist.load_data()

    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
    input_shape = (1, img_rows, img_cols)

    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')

    x_train /= 255
    x_test /= 255

    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train)
    print('x_train shape:', x_train.shape)

    print(x_train.shape[0], 'train samples')
    print(x_valid.shape[0], 'validation samples')
    print(x_test.shape[0], 'test samples')

    # Convert class vectors to binary class matrices.
    y_train = keras.utils.to_categorical(y_train, num_classes)
    y_valid = keras.utils.to_categorical(y_valid, num_classes)
    y_test = keras.utils.to_categorical(y_test, num_classes)

    # LeNet
    model = Sequential()
    model.add(Conv2D(32, kernel_size=(3, 3),
                     activation='relu',
                     input_shape=input_shape))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))

    # Let's train the model using ADAM

    model.compile(loss=keras.losses.categorical_crossentropy,
                  optimizer=keras.optimizers.Adam(),
                  metrics=['accuracy'])

    print('Using real-time data augmentation.')

    # This will do preprocessing and realtime data augmentation:
    imagegen = ImageAugmentation(fabolas_config)

    # Fit the model on the batches generated by datagen.flow().
    model.fit_generator(imagegen.apply_transform(x_train, y_train,
                                                 batch_size=batch_size),
                        steps_per_epoch=x_train.shape[0] // batch_size,
                        epochs=epochs,
                        validation_data=(x_valid, y_valid)
                        )

    # Evaluate model with test data set and share sample prediction results
    score = model.evaluate(x_valid, y_valid, verbose=0)
    print("score", score)
    # return validation error
    y = 1 - score[1]
    c = time.time() - start_time
    return y, c


# We optimize s on a log scale, as we expect that the performance varies
# logarithmically across s
s_min = 100
s_max = 50000

config_space = ImageAugmentation.get_config_space()
config = config_space.sample_configuration()

hyperparameters = config_space.get_hyperparameters()

lower = []
upper = []

for hyperparameter in hyperparameters:
    if hasattr(hyperparameter, "lower"):
        lower_bound = hyperparameter.lower
        upper_bound = hyperparameter.upper
    else:
        domain = hyperparameter.choices
        lower_bound, upper_bound = min(domain), max(domain)

    lower.append(lower_bound)
    upper.append(upper_bound)


lower = np.array(lower)
upper = np.array(upper)

# Start Fabolas to optimize the objective function
# XXX: consider only fractions s of the whole dataset in objective_function
res = fabolas(
    lambda x, s: objective_function(x=x, s=s, config_space=config_space),
    lower=lower, upper=upper, s_min=s_min, s_max=s_max, num_iterations=100
)

x_best = res["x_opt"]
print(x_best)
print(objective_function(x_best[:, :-1], s=x_best[:, None, -1]))
