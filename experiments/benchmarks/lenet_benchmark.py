#!/usr/bin/python3
# -*- coding: iso-8859-15 -*-

import time
import numpy as np

import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from sklearn.model_selection import StratifiedShuffleSplit
import ConfigSpace as CS

from autoda.data_augmentation import ImageAugmentation

import logging
logging.basicConfig(level=logging.INFO)


def augment(x, s, config_space):
    hyperparameters = config_space.get_hyperparameters()

    def hyperparameter_value(x, hyperparameter, hyperparameter_index):
        if isinstance(hyperparameter, CS.UniformFloatHyperparameter):
            value = x[hyperparameter_index]
        else:
            value = int(round(x[hyperparameter_index]))

        return value

    fabolas_mapping = {
        hyperparameter.name: hyperparameter_value(x, hyperparameter, hyperparameter_index)
        for hyperparameter_index, hyperparameter in enumerate(hyperparameters)
    }

    fabolas_config = CS.Configuration(
        configuration_space=config_space, values=fabolas_mapping
    )
    print(fabolas_mapping)
    print(fabolas_config)
    # This will do preprocessing and realtime data augmentation:
    return ImageAugmentation(fabolas_config)


def lenet_function(x, s, config_space):

    batch_size = 128
    num_classes = 10
    epochs = 12

    assert(batch_size <= s), "Batch size must be less or equal to input data size"

    start_time = time.time()

    augmenter = augment(x, s, config_space)

    # input image dimensions
    img_rows, img_cols = 28, 28

    # The data, shuffled and split between train and test sets:
    (X, y), (x_test, y_test) = mnist.load_data()

    X = X.reshape(X.shape[0], 1, img_rows, img_cols)
    # x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
    input_shape = (1, img_rows, img_cols)

    X = X.astype('float32')
    # x_test = x_test.astype('float32')

    X /= 255
    # x_test /= 255

    stratified = StratifiedShuffleSplit(n_splits=3, test_size=0.25, random_state=0)

    for train_index, valid_index in stratified.split(X, y):
        x_train, x_valid = X[train_index], X[valid_index]
        y_train, y_valid = y[train_index], y[valid_index]

    print('x_train shape:', x_train.shape)

    print(x_train.shape[0], 'train samples')
    print(x_valid.shape[0], 'validation samples')
    print(x_test.shape[0], 'test samples')

    # Convert class vectors to binary class matrices.
    y_train = keras.utils.to_categorical(y_train, num_classes)
    y_valid = keras.utils.to_categorical(y_valid, num_classes)
    # y_test = keras.utils.to_categorical(y_test, num_classes)

    s_max = y_train.shape[0]
    shuffle = np.random.permutation(np.arange(s_max))
    train_subset = x_train[shuffle[:s]]
    train_targets_subset = y_train[shuffle[:s]]

    # LeNet
    model = Sequential()
    model.add(Conv2D(32, kernel_size=(3, 3),
                     activation='relu',
                     input_shape=input_shape))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))

    # Let's train the model using ADAM

    model.compile(loss=keras.losses.categorical_crossentropy,
                  optimizer=keras.optimizers.Adam(),
                  metrics=['accuracy'])

    print('Using real-time data augmentation.')

    # Fit the model on the batches generated by datagen.flow().
    model.fit_generator(augmenter.apply_transform(train_subset, train_targets_subset,
                                                  batch_size=batch_size),
                        steps_per_epoch=train_subset.shape[0] // batch_size,
                        epochs=epochs,
                        validation_data=(x_valid, y_valid)
                        )

    # Evaluate model with test data set and share sample prediction results
    score = model.evaluate(x_valid, y_valid, verbose=0)

    # compute validation error
    y = 1 - score[1]
    c = time.time() - start_time
    return y, c
